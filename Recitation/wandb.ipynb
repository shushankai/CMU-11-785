{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cf54424",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b74c3d5",
   "metadata": {},
   "source": [
    "## Installation and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386cb8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e50fc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim \n",
    "from  torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea38d986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.3.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshushanksingh310\u001b[0m (\u001b[33mshushanksingh310-birla-institute-of-technology-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb, os \n",
    "os.environ['WANDB_API_KEY'] = \"8725ebdffefbbe249e137ef04781ca644aa5ee0e\"\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16280d7",
   "metadata": {},
   "source": [
    "## Helper functions and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19db7094",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.CIFAR10(\n",
    "    root = 'data',\n",
    "    train = True, \n",
    "    transform = ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "data_test = datasets.CIFAR10(\n",
    "    root = 'data',\n",
    "    train = False, \n",
    "    download = True, \n",
    "    transform = ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d44c6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(batch_size, data_train, data_test):\n",
    "    train_loader = torch.utils.data.DataLoader(data_train, batch_size = batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(data_test, batch_size = batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75a03679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (CNN): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): AvgPool2d(kernel_size=9, stride=9, padding=0)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (classification): Linear(in_features=576, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.CNN = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.AvgPool2d(kernel_size=9),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.classification = nn.Linear(576, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_cnn = self.CNN(x)\n",
    "        res = self.classification(x_cnn)\n",
    "        return res\n",
    "\n",
    "model = Network().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8c27987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader , test_loader = build_dataset(64, data_train, data_test)\n",
    "for x, y in train_loader:\n",
    "    break\n",
    "model(x.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "049f1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optim(optimizer, learning_rate, model):\n",
    "    if optimizer=='sgd':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e2319a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, scaler):\n",
    "    num_correct = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs - model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "        \n",
    "        total_loss += fload(loss)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "    ep_loss = float(total_loss / len(loader))\n",
    "\n",
    "    return model, ep_loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d7972150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, finish = True):\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(run_config['epochs']):\n",
    "        batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, \n",
    "        desc='Train')\n",
    "\n",
    "        num_correct = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, y)\n",
    "\n",
    "            num_correct += int((torch.argmax(outputs, axis=1) ==y).sum())\n",
    "\n",
    "            total_loss += float(loss)\n",
    "\n",
    "            batch_bar.set_postfix(\n",
    "                acc=\"{:.04f}%\".format(100 * num_correct / ((i +1) * run_config['batch_size'])),\n",
    "                loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "                num_correct = num_correct, \n",
    "                lr = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
    "            )\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            batch_bar.update()\n",
    "        batch_bar.close()\n",
    "\n",
    "        train_loss = float(total_loss/ len(train_loader))\n",
    "        train_acc = 100 * num_correct / (len(train_loader) * run_config['batch_size'])\n",
    "        lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "        print(\"Epoch {}/{}: Train Acc {:.04f}%, Train Loss {:.04f}, Learning Rate {:.04f}\".format(\n",
    "            epoch + 1, \n",
    "            run_config['epochs'],\n",
    "            train_acc,\n",
    "            train_loss, \n",
    "            lr\n",
    "        ))\n",
    "\n",
    "        # what to log \n",
    "\n",
    "        metrics = {\n",
    "            \"train_loss\": train_loss, \n",
    "            \"train_acc\": train_acc, \n",
    "            \"lr\": lr \n",
    "        }\n",
    "\n",
    "        wandb.log(metrics)\n",
    "\n",
    "        #updating the model version \n",
    "\n",
    "        if train_acc > best_acc: \n",
    "            best_acc = train_acc\n",
    "\n",
    "            # saving the model and optimizer states\n",
    "\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict()\n",
    "            }, \"Model.pth\")\n",
    "\n",
    "            # ALTERNATIVE 1: Saving files as artifacts  \n",
    "            # Creating Artifact\n",
    "            model_artifact = wandb.Artifact(run_config['model'], type='model')\n",
    "\n",
    "            model_artifact.add_file(\"Model.pth\")\n",
    "\n",
    "            run.log_artifact(model_artifact)\n",
    "\n",
    "            wandb.save(\"Model.pth\")\n",
    "\n",
    "    if finish:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "891a41a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5135/4046953894.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "run_config = {\n",
    "    'model': '1-2dcnn',\n",
    "    'optimizer':'sgd',\n",
    "    'lr': 2e-3,\n",
    "    'batch_size':64,\n",
    "    'epochs': 5\n",
    "}\n",
    "\n",
    "train_loader, test_loader = build_dataset(run_config['batch_size'], data_train, data_test)\n",
    "\n",
    "optimizer = get_optim(run_config['optimizer'], run_config['lr'], model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a8eae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁</td></tr><tr><td>train_acc</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.002</td></tr><tr><td>train_acc</td><td>31.99528</td></tr><tr><td>train_loss</td><td>1.94376</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1-2dcnn</strong> at: <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/4h9i7clk' target=\"_blank\">https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/4h9i7clk</a><br> View project at: <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test' target=\"_blank\">https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250411_212330-4h9i7clk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shushanksingh/work/CMU-11-785/Recitation/wandb/run-20250411_212409-vfuxzi07</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/vfuxzi07' target=\"_blank\">1-2dcnn</a></strong> to <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test' target=\"_blank\">https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/vfuxzi07' target=\"_blank\">https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/vfuxzi07</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    entity=\"shushanksingh310-ai\",\n",
    "    project=\"test\",\n",
    "    job_type=\"model-training\",\n",
    "    name=run_config['model'],\n",
    "    config=run_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e34cdf07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/782 [00:00<?, ?it/s]/tmp/ipykernel_5135/830930794.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: Train Acc 34.4949%, Train Loss 1.8823, Learning Rate 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: Train Acc 36.0354%, Train Loss 1.8425, Learning Rate 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: Train Acc 37.3561%, Train Loss 1.8125, Learning Rate 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: Train Acc 38.1474%, Train Loss 1.7865, Learning Rate 0.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: Train Acc 39.0825%, Train Loss 1.7673, Learning Rate 0.0020\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▅▇█</td></tr><tr><td>train_loss</td><td>█▆▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.002</td></tr><tr><td>train_acc</td><td>39.08248</td></tr><tr><td>train_loss</td><td>1.76729</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">1-2dcnn</strong> at: <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/vfuxzi07' target=\"_blank\">https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test/runs/vfuxzi07</a><br> View project at: <a href='https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test' target=\"_blank\">https://wandb.ai/shushanksingh310-birla-institute-of-technology-/test</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250411_212409-vfuxzi07/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e996f456",
   "metadata": {},
   "source": [
    "## Resume a previous run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f532f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESUME_LOGGING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2823e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3635203901.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[26], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    RESUME_LOGGING:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if RESUME_LOGGING:\n",
    "    run_id = \"\"\n",
    "    run = wandb.init(\n",
    "        id = run_id,\n",
    "        resume = \"must\"\n",
    "        project = 'test'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b012a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
